{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab5 - Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG57TEe5WlBcXOg8QsNa+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungmin001/prac_deeplearningzerotoall/blob/main/Lab5_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rp1TDWLM8RZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97b2e72-425e-4b6e-c5a1-4975b3593009"
      },
      "source": [
        "# code by devKya\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "x_train = np.array([\r\n",
        "    [1, 2],\r\n",
        "    [2, 3],\r\n",
        "    [3, 1],\r\n",
        "    [4, 3],\r\n",
        "    [5, 3],\r\n",
        "    [6, 2]], dtype=np.float32)\r\n",
        "y_train = np.array([\r\n",
        "    [0],\r\n",
        "    [0],\r\n",
        "    [0],\r\n",
        "    [1],\r\n",
        "    [1],\r\n",
        "    [1]], dtype=np.float32)\r\n",
        "\r\n",
        "x_test = np.array([[5, 2]], dtype=np.float32)\r\n",
        "y_test = np.array([[1]], dtype=np.float32)\r\n",
        "\r\n",
        "# tf.data.Dataset 파이프라인을 이용하여 값을 입력\r\n",
        "# from_tensor_slices 클래스 매서드를 사용하면 리스트, 넘파이, 텐서플로 자료형에서 데이터셋을 만들 수 있음\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train)) # x_train y_train 합쳐서 x_train열이 끝나면 y_train열이 옴. # batch : 한번에 학습하는 data의 개수\r\n",
        "W = tf.Variable(tf.zeros([2, 1]), name='weight') # (x_train열개수, 결과 개수)\r\n",
        "b = tf.Variable(tf.zeros([1]), name='bias')\r\n",
        "\r\n",
        "# 원소의 자료구조 반환\r\n",
        "dataset.element_spec\r\n",
        "\r\n",
        "def logistic_regression(features):\r\n",
        "    hypothesis = tf.sigmoid(tf.matmul(features, W) + b)\r\n",
        "    return hypothesis\r\n",
        "\r\n",
        "\r\n",
        "def loss_fn(features, labels):\r\n",
        "    hypothesis = logistic_regression(features)\r\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\r\n",
        "    return cost\r\n",
        "\r\n",
        "def grad(hypothesis, features, labels):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        loss_value = loss_fn(features, labels)\r\n",
        "    return tape.gradient(loss_value, [W,b])\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
        "\r\n",
        "EPOCHS = 3000\r\n",
        "\r\n",
        "for step in range(EPOCHS + 1):\r\n",
        "    for features, labels in iter(dataset):\r\n",
        "        hypothesis = logistic_regression(features)\r\n",
        "        grads = grad(hypothesis, features, labels)\r\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads, [W,b]))\r\n",
        "        if step % 300 == 0:\r\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(features, labels)))\r\n",
        "            # print(hypothesis)\r\n",
        "            \r\n",
        "def accuracy_fn(hypothesis, labels):\r\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\r\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\r\n",
        "print('Accuracy: {}%'.format(test_acc * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.6874\n",
            "Iter: 300, Loss: 0.5054\n",
            "Iter: 600, Loss: 0.4535\n",
            "Iter: 900, Loss: 0.4228\n",
            "Iter: 1200, Loss: 0.3992\n",
            "Iter: 1500, Loss: 0.3790\n",
            "Iter: 1800, Loss: 0.3608\n",
            "Iter: 2100, Loss: 0.3442\n",
            "Iter: 2400, Loss: 0.3288\n",
            "Iter: 2700, Loss: 0.3146\n",
            "Iter: 3000, Loss: 0.3013\n",
            "Accuracy: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cP_fLdVlxKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df742a49-536a-420a-ebe4-7985191335fa"
      },
      "source": [
        "# my code\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Data\r\n",
        "x_train = np.array([\r\n",
        "    [1, 2],\r\n",
        "    [2, 3],\r\n",
        "    [3, 1],\r\n",
        "    [4, 3],\r\n",
        "    [5, 3],\r\n",
        "    [6, 2]], dtype=np.float32)\r\n",
        "y_train = np.array([\r\n",
        "    [0],\r\n",
        "    [0],\r\n",
        "    [0],\r\n",
        "    [1],\r\n",
        "    [1],\r\n",
        "    [1]], dtype=np.float32)\r\n",
        "\r\n",
        "x_test = np.array([[5, 2]], dtype=np.float32)\r\n",
        "y_test = np.array([[1]], dtype=np.float32)\r\n",
        "\r\n",
        "# Dataset\r\n",
        "dataset= tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n",
        "print(dataset)\r\n",
        "dataset= dataset.batch(len(x_train))\r\n",
        "print(dataset)\r\n",
        "\r\n",
        "\r\n",
        "# Weight, bias\r\n",
        "W= tf.Variable(tf.random.normal([2,1], dtype=np.float32, name='Weight'))\r\n",
        "b= tf.Variable(tf.random.normal([1], dtype=np.float32, name='bias'))\r\n",
        "\r\n",
        "# logistic regression\r\n",
        "def logistic_regression(features):\r\n",
        "    # sigmoid = 1/(1+e^(-z)) , z = X*W + b\r\n",
        "    hypothesis= tf.sigmoid( tf.matmul(features,W) + b ) \r\n",
        "    return hypothesis\r\n",
        "\r\n",
        "# cost function\r\n",
        "def cost_func(features, labels):\r\n",
        "    hypothesis = logistic_regression(features)\r\n",
        "    # cost = y * - log(h(x)) + (1-y) * - log(1 - h(x)) # y=1일 때 cost 0되는 함수+ y=0일 때 cost 0되는 함수 온전하게 반영\r\n",
        "    # cost function은 cost의 reduce_mean\r\n",
        "    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\r\n",
        "    return cost\r\n",
        "\r\n",
        "def grad(features, labels):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        cost = cost_func(features, labels)\r\n",
        "    return tape.gradient(cost, [W,b])\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
        "\r\n",
        "EPOCHS= 3000\r\n",
        "# 반복 learning\r\n",
        "for step in range(EPOCHS + 1):\r\n",
        "    for features, labels in dataset: # features shape=(6,2) labels shape=(6,1)\r\n",
        "        grads= grad(features, labels)\r\n",
        "        optimizer.apply_gradients(grads_and_vars = zip(grads, [W,b]))\r\n",
        "        \r\n",
        "        if step % 300 == 0:\r\n",
        "            print(\"{:5} | {:12.4f}\".format(step, cost_func(features,labels)))\r\n",
        "\r\n",
        "# 예측값, 실제값 비교해서 정확성 반환\r\n",
        "def accuracy_fn(hypothesis, labels):\r\n",
        "    # 0.5 기준 크면 1 작으면 0\r\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\r\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "test_acc = accuracy_fn(logistic_regression(x_test), y_test)\r\n",
        "print('Accuracy: {}%'.format(test_acc * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: ((2,), (1,)), types: (tf.float32, tf.float32)>\n",
            "<BatchDataset shapes: ((None, 2), (None, 1)), types: (tf.float32, tf.float32)>\n",
            "    0 |       6.7401\n",
            "  300 |       0.5137\n",
            "  600 |       0.4326\n",
            "  900 |       0.3963\n",
            " 1200 |       0.3725\n",
            " 1500 |       0.3534\n",
            " 1800 |       0.3368\n",
            " 2100 |       0.3218\n",
            " 2400 |       0.3079\n",
            " 2700 |       0.2952\n",
            " 3000 |       0.2833\n",
            "Accuracy: 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OvJa5PP7YvO"
      },
      "source": [
        "#### 참고\r\n",
        "batch o iter o shape=(6, 2) shape=(6, 1)  \r\n",
        "batch x iter o shape=[2] (행이 없고 x 2개씩) matrix가 아니라 matmul연산안됨  \r\n",
        "batch o iter x shape (6,2)  \r\n",
        "batch x iter x shape=[2] matmul 오류  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-t556aVeR37",
        "outputId": "33537964-c084-432a-928a-00905cfff941"
      },
      "source": [
        "# dataset 기초\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# ndarray x 선언\r\n",
        "x=np.array([[1.,2.],[3.,4.]])\r\n",
        "print(x.dtype)\r\n",
        "print(len(x))\r\n",
        "\r\n",
        "# dataset 선언\r\n",
        "dataset=tf.data.Dataset.from_tensor_slices(x)\r\n",
        "dataset2=tf.data.Dataset.from_tensors(x)\r\n",
        "\r\n",
        "print(dataset.element_spec) # 각 element 들의 속성\r\n",
        "\r\n",
        "print(dataset) # slices는 잘라서 저장\r\n",
        "print(dataset2) # 한꺼번에 저장"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float64\n",
            "2\n",
            "TensorSpec(shape=(2,), dtype=tf.float64, name=None)\n",
            "<TensorSliceDataset shapes: (2,), types: tf.float64>\n",
            "<TensorDataset shapes: (2, 2), types: tf.float64>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}