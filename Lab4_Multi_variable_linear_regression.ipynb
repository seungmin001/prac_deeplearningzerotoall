{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4 - Multi-variable linear regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPESGKv05ZuPMS1hF9NZ0g2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungmin001/prac_deeplearningzerotoall/blob/main/Lab4_Multi_variable_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZC-MGctOn2h"
      },
      "source": [
        "# 일반식 (multi variable)\r\n",
        "\r\n",
        "**y= w1 * x1 + w2 * x2 + ... + wn * xn + b**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO0bi0MeOaTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6720b0-5827-4131-c4eb-3c4fa74672f6"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Data\r\n",
        "x1 = [ 73., 93., 89., 96., 73.]\r\n",
        "x2 = [ 80., 88., 91., 98., 66.]\r\n",
        "x3 = [ 75., 93., 90.,100., 70.]\r\n",
        "Y  = [152.,185.,180.,196.,142.]\r\n",
        "\r\n",
        "# weight, bias\r\n",
        "w1 = tf.Variable(tf.random.normal([1]))\r\n",
        "w2 = tf.Variable(tf.random.normal([1]))\r\n",
        "w3 = tf.Variable(tf.random.normal([1]))\r\n",
        "b = tf.Variable(tf.random.normal([1]))\r\n",
        "\r\n",
        "learning_rate = 0.000001\r\n",
        "\r\n",
        "for i in range(1000+1):\r\n",
        "    # tf.GradientTape() to record the gradient of the cost function\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\r\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\r\n",
        "    # calculates gradient of the cost\r\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1,w2,w3,b])\r\n",
        "\r\n",
        "    # update weight, bias\r\n",
        "    w1.assign_sub(learning_rate * w1_grad)\r\n",
        "    w2.assign_sub(learning_rate * w2_grad)\r\n",
        "    w3.assign_sub(learning_rate * w3_grad)\r\n",
        "    b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "    if i%50 == 0:\r\n",
        "        print(\"{:5} | {:12.4f}\".format(i,cost.numpy()))\r\n",
        "\r\n",
        "print(cost)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |   63701.5312\n",
            "   50 |     731.0312\n",
            "  100 |      32.2508\n",
            "  150 |      24.4397\n",
            "  200 |      24.2956\n",
            "  250 |      24.2368\n",
            "  300 |      24.1792\n",
            "  350 |      24.1218\n",
            "  400 |      24.0644\n",
            "  450 |      24.0073\n",
            "  500 |      23.9503\n",
            "  550 |      23.8934\n",
            "  600 |      23.8367\n",
            "  650 |      23.7801\n",
            "  700 |      23.7236\n",
            "  750 |      23.6673\n",
            "  800 |      23.6112\n",
            "  850 |      23.5551\n",
            "  900 |      23.4992\n",
            "  950 |      23.4436\n",
            " 1000 |      23.3880\n",
            "tf.Tensor(23.388014, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX03swkgYaWy"
      },
      "source": [
        "# Matrix 사용\r\n",
        "\r\n",
        "**H(x) = X * W**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQIB6ZAZXoqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba91d9ae-36ad-4d6f-eb44-dcf8685bd4ad"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Data\r\n",
        "data = np.array([\r\n",
        "    [ 73., 80., 75.,152.],\r\n",
        "    [ 93., 88., 93.,185.],\r\n",
        "    [ 89., 91., 90.,180.],\r\n",
        "    [ 96., 98.,100.,196.],\r\n",
        "    [ 73., 66., 70.,142.]\r\n",
        "], dtype=np.float32) # dtype 정해줘야 하나? > matmul에서 W의 random normal 의 기본 float32이기 때문에 맞춰줘야한다.\r\n",
        "\r\n",
        "# slice data\r\n",
        "X = data[:,:-1]\r\n",
        "Y = data[:, [-1]]\r\n",
        "\r\n",
        "# weight, bias\r\n",
        "W = tf.Variable(tf.random.normal([X.shape[1],1]))\r\n",
        "b = tf.Variable(tf.random.normal([1]))\r\n",
        "\r\n",
        "learning_rate= 0.000001\r\n",
        "\r\n",
        "# hypothesis, prediction func\r\n",
        "def predict(X):\r\n",
        "    return tf.matmul(X,W) + b\r\n",
        "\r\n",
        "n_epochs = 2000\r\n",
        "\r\n",
        "# repetition\r\n",
        "for i in range(n_epochs+1):\r\n",
        "\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        cost = tf.reduce_mean( tf.square( predict(X) - Y))\r\n",
        "    \r\n",
        "    # calculate gradient\r\n",
        "    W_grad, b_grad = tape.gradient(cost , [W, b])\r\n",
        "\r\n",
        "    # update\r\n",
        "    W.assign_sub(learning_rate * W_grad)\r\n",
        "    b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "    if i % 100 == 0:\r\n",
        "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 |   87119.7031\n",
            "  100 |      18.2759\n",
            "  200 |       7.5139\n",
            "  300 |       7.4748\n",
            "  400 |       7.4373\n",
            "  500 |       7.3999\n",
            "  600 |       7.3627\n",
            "  700 |       7.3258\n",
            "  800 |       7.2890\n",
            "  900 |       7.2525\n",
            " 1000 |       7.2161\n",
            " 1100 |       7.1799\n",
            " 1200 |       7.1439\n",
            " 1300 |       7.1081\n",
            " 1400 |       7.0725\n",
            " 1500 |       7.0371\n",
            " 1600 |       7.0019\n",
            " 1700 |       6.9668\n",
            " 1800 |       6.9319\n",
            " 1900 |       6.8972\n",
            " 2000 |       6.8628\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}